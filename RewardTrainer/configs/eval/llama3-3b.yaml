prompt: cot
vllm_args:
  model_args:
    model: output/Llama-3.2-3B-Instruct-arena_cot_distill_verified_lr_1e-5_epoch_3
    dtype: float16
  sampling_params:
    temperature: 0
    max_tokens: 512
hf_args:
  model_args:
    model: meta-llama/Meta-Llama-3-3B-Instruct
    dtype: float16
  generate_kwargs:
    max_new_tokens: 20
    pad_token_id: 128001
    do_sample: false
    temperature: 0
