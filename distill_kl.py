# -*- coding: utf-8 -*-
import argparse
import json
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
)
from tqdm import tqdm


def safe_item(x):
    return x.item() if isinstance(x, torch.Tensor) else float(x)


# ====================== æ•°æ®é›† ======================
class JsonlDataset(Dataset):
    def __init__(self, tokenizer, file_path, max_length=512, teacher_tokenizer=None):
        self.examples = []
        self.tokenizer = tokenizer
        self.teacher_tokenizer = teacher_tokenizer

        # é¦–å…ˆç»Ÿè®¡æ–‡ä»¶è¡Œæ•°ä»¥æ˜¾ç¤ºè¿›åº¦æ¡
        with open(file_path, "r", encoding="utf-8") as f:
            line_count = sum(1 for _ in f)

        # å¤„ç†æ•°æ®å¹¶æ˜¾ç¤ºè¿›åº¦æ¡
        with open(file_path, "r", encoding="utf-8") as f:
            for line in tqdm(f, total=line_count, desc="å¤„ç†è®­ç»ƒæ•°æ®"):
                data = json.loads(line)
                messages = [
                    {"role": "user", "content": data["question"]},
                    {"role": "assistant", "content": data["response"]},
                ]

                # å­¦ç”Ÿæ¨¡å‹è¾“å…¥
                student_text = tokenizer.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=False
                )
                student_tokenized = tokenizer(
                    student_text,
                    truncation=True,
                    max_length=max_length,
                    padding="max_length",
                )

                # æ‰¾åˆ°å­¦ç”Ÿæ¨¡å‹çš„ assistant å¼€å§‹ä½ç½®
                user_only_text = tokenizer.apply_chat_template(
                    [{"role": "user", "content": data["question"]}],
                    tokenize=False,
                    add_generation_prompt=True,
                )
                user_only_ids = tokenizer(
                    user_only_text,
                    add_special_tokens=False,
                    truncation=True,
                    max_length=max_length,
                )["input_ids"]
                student_prefix_len = len(user_only_ids)

                # æ•™å¸ˆæ¨¡å‹è¾“å…¥
                if teacher_tokenizer is not None:
                    teacher_text = teacher_tokenizer.apply_chat_template(
                        messages,
                        tokenize=False,
                        add_generation_prompt=False,
                        enable_thinking=False,
                    )
                    teacher_tokenized = teacher_tokenizer(
                        teacher_text,
                        truncation=True,
                        max_length=max_length,
                        padding="max_length",
                    )

                    # æ‰¾åˆ°æ•™å¸ˆæ¨¡å‹çš„ assistant å¼€å§‹ä½ç½®
                    teacher_user_only_text = teacher_tokenizer.apply_chat_template(
                        [{"role": "user", "content": data["question"]}],
                        tokenize=False,
                        add_generation_prompt=True,
                        enable_thinking=False,
                    )
                    teacher_user_only_ids = teacher_tokenizer(
                        teacher_user_only_text,
                        add_special_tokens=False,
                        truncation=True,
                        max_length=max_length,
                    )["input_ids"]
                    teacher_prefix_len = len(teacher_user_only_ids)
                else:
                    teacher_tokenized = student_tokenized
                    teacher_prefix_len = student_prefix_len

                # æ„é€  labelsï¼ˆåŸºäºå­¦ç”Ÿæ¨¡å‹ï¼‰
                labels = student_tokenized["input_ids"][:]
                labels[:student_prefix_len] = [-100] * student_prefix_len

                # ä¿å­˜æ‰€æœ‰å­—æ®µï¼ŒåŒ…æ‹¬å‰ç¼€é•¿åº¦
                example = {
                    "input_ids": student_tokenized["input_ids"],
                    "attention_mask": student_tokenized["attention_mask"],
                    "labels": labels,
                    "teacher_input_ids": teacher_tokenized["input_ids"],
                    "teacher_attention_mask": teacher_tokenized["attention_mask"],
                    "student_prefix_len": student_prefix_len,
                    "teacher_prefix_len": teacher_prefix_len,
                }
                self.examples.append(example)

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, idx):
        ex = self.examples[idx]
        return {
            "input_ids": ex["input_ids"],
            "attention_mask": ex["attention_mask"],
            "labels": ex["labels"],
            "teacher_input_ids": ex["teacher_input_ids"],
            "teacher_attention_mask": ex["teacher_attention_mask"],
            "student_prefix_len": ex["student_prefix_len"],
            "teacher_prefix_len": ex["teacher_prefix_len"],
        }


# ====================== Trainer ======================
class DistillTrainer(Trainer):
    def __init__(
        self,
        teacher_model,
        student_vocab_size,
        temperature=1.0,
        lm_weight=1.0,
        kl_weight=0.0,
        teacher_tokenizer=None,
        *args,
        **kwargs,
    ):
        self.teacher_tokenizer = teacher_tokenizer
        super().__init__(*args, **kwargs)

        self.teacher = teacher_model
        if self.teacher is not None:
            self.teacher.eval()
            for p in self.teacher.parameters():
                p.requires_grad = False

        self.student_vocab_size = student_vocab_size
        self.temperature = temperature
        self.eps = 1e-9
        self.lm_weight = lm_weight
        self.kl_weight = kl_weight

        # ä¿å­˜ tokenizer å’Œ datasetï¼Œç”¨äºç”Ÿæˆæµ‹è¯•
        self.tokenizer = kwargs.get("tokenizer", None)
        self.train_dataset = kwargs.get("train_dataset", None)
        if self.tokenizer is None:
            raise ValueError("è¯·ä¼ å…¥ tokenizer åˆ° Trainerï¼")

    # ---------- è®¾å¤‡åŒæ­¥ ----------
    def _sync_teacher_device(self, model):
        if self.teacher is None:
            return
        try:
            model_device = next(model.parameters()).device
        except StopIteration:
            model_device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        if hasattr(self.teacher, "device") and self.teacher.device != model_device:
            self.teacher.to(model_device)
        else:
            for p in self.teacher.parameters():
                if p.device != model_device:
                    self.teacher.to(model_device)
                    break
            for b in self.teacher.buffers():
                if b.device != model_device:
                    self.teacher.to(model_device)
                    break

    # ---------- è®¡ç®— loss ----------

    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        device = next(model.parameters()).device
        input_ids = inputs["input_ids"].to(device)
        attention_mask = inputs.get("attention_mask", None)

        if attention_mask is not None:
            attention_mask = attention_mask.to(device)

        # å…ˆè·å– labels
        labels = inputs["labels"].to(device)

        batch_size = input_ids.shape[0]

        # è·å–é¢„è®¡ç®—çš„å‰ç¼€é•¿åº¦
        student_prefix_lens = inputs["student_prefix_len"]
        teacher_prefix_lens = inputs["teacher_prefix_len"]

        # åª forward ä¸€æ¬¡å­¦ç”Ÿæ¨¡å‹
        s_out = model(input_ids=input_ids, attention_mask=attention_mask)
        student_logits = s_out.logits

        # ----------- KL(teacher||student) ----------
        kl_loss = 0.0
        if self.kl_weight > 0 and self.teacher is not None:
            self._sync_teacher_device(model)

            teacher_input_ids = inputs["teacher_input_ids"].to(device)
            teacher_attention_mask = inputs.get("teacher_attention_mask", None)
            if teacher_attention_mask is not None:
                teacher_attention_mask = teacher_attention_mask.to(device)

            with torch.no_grad():
                t_out = self.teacher(
                    input_ids=teacher_input_ids, attention_mask=teacher_attention_mask
                )
                teacher_logits = t_out.logits

            T = float(self.temperature)

            # å¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡Œå‰ç¼€æ©ç å’Œå¯¹é½
            batch_kl_losses = []

            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            if self.args.local_rank in (-1, 0) and self.state.global_step % 5 == 0:
                print("\nKL æ•£åº¦è®¡ç®—è°ƒè¯•ä¿¡æ¯:")

            for b in range(batch_size):
                teacher_start = teacher_prefix_lens[b]
                student_start = student_prefix_lens[b]

                # æˆªå–å¯å¯¹é½é•¿åº¦
                teacher_available_len = teacher_logits.shape[1] - teacher_start - 1
                student_available_len = student_logits.shape[1] - student_start - 1
                min_len = min(teacher_available_len, student_available_len)

                if min_len <= 0:
                    batch_kl_losses.append(torch.tensor(0.0, device=device))
                    continue

                teacher_slice = teacher_logits[
                    b : b + 1, teacher_start : teacher_start + min_len, :
                ]
                student_slice = student_logits[
                    b : b + 1, student_start : student_start + min_len, :
                ]

                # è°ƒè¯•ï¼šæ£€æŸ¥åˆ‡ç‰‡çš„ç»Ÿè®¡ä¿¡æ¯
                if (
                    self.args.local_rank in (-1, 0)
                    and self.state.global_step % 5 == 0
                    and b == 0
                ):
                    print(
                        f"  æ ·æœ¬{b}: teacher_slice shape={teacher_slice.shape}, student_slice shape={student_slice.shape}"
                    )
                    print(
                        f"  teacher logits ç»Ÿè®¡: mean={teacher_slice.mean().item():.4f}, std={teacher_slice.std().item():.4f}"
                    )
                    print(
                        f"  student logits ç»Ÿè®¡: mean={student_slice.mean().item():.4f}, std={student_slice.std().item():.4f}"
                    )

                t_log_prob = F.log_softmax(teacher_slice / T, dim=-1)
                s_log_prob = F.log_softmax(student_slice / T, dim=-1)
                t_prob = torch.exp(t_log_prob)

                per_elem = t_prob * (t_log_prob - s_log_prob)
                per_token_kl = per_elem.sum(dim=-1)

                # ä½¿ç”¨ labels è¿›è¡Œ mask
                shift_labels = labels[
                    b : b + 1, student_start + 1 : student_start + 1 + min_len
                ]
                kl_mask = (shift_labels != -100).to(per_token_kl.dtype)

                total_nonpad = kl_mask.sum()

                # è°ƒè¯•ï¼šæ£€æŸ¥ KL æ•£åº¦å€¼
                if (
                    self.args.local_rank in (-1, 0)
                    and self.state.global_step % 5 == 0
                    and b == 0
                ):
                    print(f"  per_token_kl å‰ 10 ä¸ªå€¼: {per_token_kl[0, :10].tolist()}")
                    print(f"  kl_mask å‰ 10 ä¸ªå€¼: {kl_mask[0, :10].tolist()}")
                    print(f"  æœ‰æ•ˆ token æ•°: {total_nonpad.item()}/{min_len}")

                    # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å¤§çš„ KL å€¼
                    max_kl = per_token_kl.max().item()
                    mean_kl = per_token_kl.mean().item()
                    print(f"  KL æ•£åº¦ç»Ÿè®¡: max={max_kl:.4f}, mean={mean_kl:.4f}")

                    # æ£€æŸ¥æ•™å¸ˆå’Œå­¦ç”Ÿçš„é¢„æµ‹æ˜¯å¦è¿‡äºä¸åŒ
                    t_argmax = teacher_slice[0, :10].argmax(-1)
                    s_argmax = student_slice[0, :10].argmax(-1)
                    print(f"  æ•™å¸ˆå‰ 10 ä¸ª token é¢„æµ‹: {t_argmax.tolist()}")
                    print(f"  å­¦ç”Ÿå‰ 10 ä¸ª token é¢„æµ‹: {s_argmax.tolist()}")

                    # æ£€æŸ¥æ¦‚ç‡åˆ†å¸ƒçš„ç†µ
                    t_entropy = -(t_prob[0, :10] * t_log_prob[0, :10]).sum(-1)
                    s_entropy = -(
                        torch.exp(s_log_prob[0, :10]) * s_log_prob[0, :10]
                    ).sum(-1)
                    print(f"  æ•™å¸ˆç†µ: {t_entropy.mean().item():.4f}")
                    print(f"  å­¦ç”Ÿç†µ: {s_entropy.mean().item():.4f}")

                if total_nonpad.item() == 0:
                    sample_kl_loss = torch.tensor(0.0, device=device)
                else:
                    sample_kl_loss = (per_token_kl * kl_mask).sum() / (
                        total_nonpad + self.eps
                    )

                batch_kl_losses.append(sample_kl_loss * (T * T))

            if batch_kl_losses:
                kl_loss = torch.stack(batch_kl_losses).mean()
            else:
                kl_loss = torch.tensor(0.0, device=device)

        # ----------- LM-SFT loss ----------
        lm_loss = 0.0
        if self.lm_weight > 0:
            outputs = model(
                input_ids=inputs["input_ids"],
                attention_mask=inputs.get("attention_mask", None),
                labels=labels,
            )
            lm_loss = outputs.loss

        # ----------- åŠ æƒ ----------
        loss = self.lm_weight * lm_loss + self.kl_weight * kl_loss
        # åœ¨è°ƒè¯•éƒ¨åˆ†æ·»åŠ æ›´å¤šæ£€æŸ¥
        if self.args.local_rank in (-1, 0):
            print(f"Loss requires_grad: {loss.requires_grad}")
            b = 0
            # æ‰¾åˆ° answer å¼€å§‹ä½ç½®
            ans_start = (labels[b] != -100).nonzero(as_tuple=True)[0]
            if ans_start.numel() > 0:
                ans_start = ans_start[0].item()
                L = 15
                print("\n" + "-" * 80)
                print(f">>> STEP {self.state.global_step}  ANSWER åŒºæ®µ  <<<")

                # 1. æ£€æŸ¥æ¨¡å‹è®¾å¤‡å’Œæ•°æ®ç±»å‹
                print(f"Model device: {next(model.parameters()).device}")
                print(f"Model dtype: {next(model.parameters()).dtype}")
                print(f"Input device: {input_ids.device}")
                print(f"Logits shape: {student_logits.shape}")
                print(f"Logits dtype: {student_logits.dtype}")

                # 2. æ£€æŸ¥ logits çš„ç»Ÿè®¡ä¿¡æ¯
                print(
                    f"Logits stats - mean: {student_logits.mean().item():.4f}, std: {student_logits.std().item():.4f}"
                )
                print(
                    f"Logits min: {student_logits.min().item():.4f}, max: {student_logits.max().item():.4f}"
                )

                # 3. æ­£ç¡®å¤„ç† shift
                if ans_start > 0:  # ç¡®ä¿ä¸ä¼šè¶Šç•Œ
                    # logits[i] é¢„æµ‹ input_ids[i+1]
                    print(
                        "input_ids (è¢«é¢„æµ‹):",
                        input_ids[b, ans_start : ans_start + L].tolist(),
                    )
                    print(
                        "labels           :",
                        labels[b, ans_start : ans_start + L].tolist(),
                    )

                    # ä½¿ç”¨ ans_start-1 çš„ logits æ¥é¢„æµ‹ ans_start çš„ token
                    logits_slice = student_logits[b, ans_start - 1 : ans_start + L - 1]
                    probs = F.softmax(logits_slice, dim=-1)

                    # æ£€æŸ¥ softmax åçš„æ¦‚ç‡
                    print(f"Probs shape: {probs.shape}")
                    print(f"Probs sum (should be ~1.0): {probs[0].sum().item():.4f}")

                    # æ¨¡å‹é¢„æµ‹çš„ token
                    pred_ids = logits_slice.argmax(-1)

                    # è·å–çœŸå®æ ‡ç­¾çš„æ¦‚ç‡
                    target_ids = input_ids[b, ans_start : ans_start + L]
                    label_probs = []
                    for i in range(min(L, len(target_ids))):
                        if i < probs.shape[0]:
                            prob = probs[i, target_ids[i]].item()
                            label_probs.append(prob)

                    print("æ¨¡å‹é¢„æµ‹     :", pred_ids.tolist()[: len(label_probs)])
                    print("çœŸå®æ ‡ç­¾æ¦‚ç‡ :", [f"{p:.3f}" for p in label_probs])

                    # 4. æ£€æŸ¥å‰å‡ ä¸ªæœ€é«˜æ¦‚ç‡çš„é¢„æµ‹
                    top_k = 5
                    top_probs, top_indices = torch.topk(probs[0], top_k)
                    print(f"Top {top_k} predictions for first position:")
                    print(f"  Indices: {top_indices.tolist()}")
                    print(f"  Probs: {[f'{p:.3f}' for p in top_probs.tolist()]}")
                    print(f"  Target token: {target_ids[0].item()}")

                print("-" * 80 + "\n")
        # ---------- è°ƒè¯•ä¿¡æ¯ ----------
        if self.args.local_rank in (-1, 0):
            from datetime import datetime

            ts = datetime.now().strftime("%H:%M:%S")
            print(
                f"[{ts}] step={self.state.global_step}  "
                f"lm_loss={safe_item(lm_loss):.6f}  kl_loss={safe_item(kl_loss):.6f}  "
                f"total_loss={safe_item(loss):.6f}"
            )

            # æ¯ 10 æ­¥æ‰“å°è¯¦ç»†è°ƒè¯•ä¿¡æ¯
            if self.state.global_step % 10 == 0:
                # æ£€æŸ¥ labels ä¸­é -100 çš„æ•°é‡
                valid_labels = (inputs["labels"] != -100).sum().item()
                total_labels = inputs["labels"].numel()
                print(
                    f"Valid labels: {valid_labels}/{total_labels} ({valid_labels/total_labels*100:.1f}%)"
                )

                # æ£€æŸ¥å‰ç¼€é•¿åº¦å·®å¼‚
                prefix_diff = [
                    t - s
                    for t, s in zip(
                        teacher_prefix_lens.tolist(), student_prefix_lens.tolist()
                    )
                ]
                print(f"å‰ç¼€é•¿åº¦å·®å¼‚: {prefix_diff[:5]}...")

        if (
            self.kl_weight > 0
            and self.teacher is not None
            and self.state.global_step % 10 == 0
        ):
            print(
                "ğŸ§‘â€ğŸ« Teacher æœ€å 5 ä¸ªä½ç½® argmax:",
                teacher_logits[0, -5:].argmax(-1).tolist(),
            )
            print(
                "ğŸ§‘â€ğŸ“ Student æœ€å 5 ä¸ªä½ç½® argmax:",
                student_logits[0, -5:].argmax(-1).tolist(),
            )

        # ======  å¯¹é½æ–‡æœ¬æ ¸å¯¹  ======
        if (
            self.args.local_rank in (-1, 0)
            and self.state.global_step % 10 == 0
            and self.kl_weight > 0
        ):
            b = 0  # æ‰“å°ç¬¬ä¸€ä¸ªæ ·æœ¬
            teacher_start = teacher_prefix_lens[b]
            student_start = student_prefix_lens[b]

            teacher_available_len = teacher_logits.shape[1] - teacher_start - 1
            student_available_len = student_logits.shape[1] - student_start - 1
            min_len = min(teacher_available_len, student_available_len)

            print("\n" + "=" * 80)
            print(
                f"step={self.state.global_step}  teacher_start={teacher_start}, student_start={student_start}, min_len={min_len}"
            )

            if min_len > 0:
                t_text = self.teacher_tokenizer.decode(
                    teacher_input_ids[
                        b, teacher_start : teacher_start + min(20, min_len)
                    ],
                    skip_special_tokens=False,
                )
                s_text = self.tokenizer.decode(
                    input_ids[b, student_start : student_start + min(20, min_len)],
                    skip_special_tokens=False,
                )
                print("æ•™å¸ˆç‰‡æ®µï¼ˆå‰ 20 ä¸ª tokenï¼‰ï¼š", repr(t_text))
                print("å­¦ç”Ÿç‰‡æ®µï¼ˆå‰ 20 ä¸ª tokenï¼‰ï¼š", repr(s_text))
            print("=" * 80 + "\n")

        return (loss, s_out.logits) if return_outputs else loss


# ====================== main ======================
def main(
    student_name,
    teacher_name,
    data_path,
    output_dir,
    ds_config,
    num_epochs=1,
    batch_size=2,
    gradient_accumulation_steps=1,
    max_length=512,
    lm_weight=1.0,
    kl_weight=0.0,
):
    tokenizer = AutoTokenizer.from_pretrained(student_name, use_fast=True)
    student = AutoModelForCausalLM.from_pretrained(
        student_name, torch_dtype=torch.float16
    )

    # ----------- æŒ‰éœ€åŠ è½½æ•™å¸ˆæ¨¡å‹ ----------
    teacher = None
    teacher_tokenizer = None
    if kl_weight > 0:
        print(f"kl_weight={kl_weight}ï¼Œæ­£åœ¨åŠ è½½æ•™å¸ˆæ¨¡å‹ï¼š{teacher_name}")
        teacher = AutoModelForCausalLM.from_pretrained(
            teacher_name, torch_dtype=torch.float16
        ).eval()
        teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_name, use_fast=True)
    else:
        print("kl_weight=0ï¼Œè·³è¿‡æ•™å¸ˆæ¨¡å‹åŠ è½½ï¼Œä»…åš SFT")

    dataset = JsonlDataset(
        tokenizer, data_path, max_length=max_length, teacher_tokenizer=teacher_tokenizer
    )

    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=num_epochs,
        per_device_train_batch_size=batch_size,
        learning_rate=1.0e-6,
        logging_steps=1,
        save_strategy="steps",
        save_total_limit=1,
        bf16=True,
        deepspeed=ds_config,
        gradient_checkpointing=True,
        remove_unused_columns=False,
        report_to="none",
        gradient_accumulation_steps=gradient_accumulation_steps,
    )

    vocab_size = len(tokenizer.get_vocab())
    trainer = DistillTrainer(
        model=student,
        teacher_model=teacher,
        student_vocab_size=vocab_size,
        lm_weight=lm_weight,
        kl_weight=kl_weight,
        temperature=1.0,
        args=training_args,
        train_dataset=dataset,
        tokenizer=tokenizer,
        teacher_tokenizer=teacher_tokenizer,
    )

    trainer.train()


# ====================== å¯åŠ¨ ======================
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--student_model", type=str, required=True)
    parser.add_argument("--teacher_model", type=str, default="")
    parser.add_argument("--data_path", type=str, required=True)
    parser.add_argument("--output_dir", type=str, default="./distilled_student")
    parser.add_argument(
        "--ds_config", type=str, required=True, help="DeepSpeed é…ç½® JSON"
    )
    parser.add_argument("--num_epochs", type=int, default=1)
    parser.add_argument("--batch_size", type=int, default=1)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1)
    parser.add_argument("--max_length", type=int, default=1024)
    parser.add_argument("--lm_weight", type=float, default=0.0, help="SFT loss æƒé‡")
    parser.add_argument("--kl_weight", type=float, default=1.0, help="KL loss æƒé‡")
    args = parser.parse_args()

    main(
        args.student_model,
        args.teacher_model,
        args.data_path,
        args.output_dir,
        args.ds_config,
        num_epochs=args.num_epochs,
        batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        max_length=args.max_length,
        lm_weight=args.lm_weight,
        kl_weight=args.kl_weight,
    )
